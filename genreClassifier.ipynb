{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "### Main objective \n",
    "Our project focuses on developing a machine learning model that can classify music based on its genre. \n",
    "\n",
    "For this, we are going to use an existing dataset: the GTZAN dataset. This dataset consists of 10 different genres, each containing 100 audio samples, along with two csv files containing the features extracted from the audio files. By using an existing dataset, we can focus on analyzing its structure qnd understanding the types of features extracted. This knowledge will be valuable if we decide to build our own audio dataset in the future, as we will have a better understanding of how to create a well-structured dataset.\n",
    "\n",
    "For the machine learning model itself, we will use the Scikit-Learn Python module. This is a simple and efficient library for data analysis, pre-processing data, and model selection. It is built on SciPy, NumPy, Joblib and Threadpoolctl.\n",
    "\n",
    "### Sub-objective\n",
    "Our first sub-objective is to build a user interface that allows the users to upload a music track and get the predicted genre. This user interface will be developed using the Streamlit Python module. Streamlit is a Python library that enables the developers to create interactive and uder-friendly web applications for data-driven projects.\n",
    "\n",
    "Our second sub-objective is to compare the performance of different machine learning models with deep learning models. For this, we are going to use the TensorFlow Python module. Tensorflow is an open-source library for high-performance numerical computation. It is commonly used for machine learning applications, such as neural networks. TensorFlow can train and run deep neural networks, which can be leveraged to develop various AI applications.\n",
    "\n",
    "### Problem statement\n",
    "Correctly classifying music by genre is not always straightforward. Different pieces of music often share overlapping characteristics and can be subjectively assessed. Our project aims to develop an understanding of the relevant features that can be extracted from music and how these features can be used to create a machine learning model that can classify music by genre.\n",
    "\n",
    "For music enthusiasts, researchers, and developers, such a model can be valuable for faster categorization and analysis of music. While there is no specific target audience, this technology has broad applications in music streaming, recommendation systems, and music archiving.\n",
    "\n",
    "![DALLEImage](DALLEImage.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "### Preliminary research\n",
    "To establish a solid foundation for our project, we researched existing projects that classify music by genre. We found that music genre classification is a well-researched area in machine learning. Several studies have used audio features to classify music genres with high accuracy. We also found that the GTZAN dataset is a popular dataset for music genre classification. It contains 1000 audio tracks, each labeled with a genre.\n",
    "Several projects we have found and based on them we will develop our project:\n",
    "- [Work w/ Audio Data: Visualise, Classify, Recommend](https://www.kaggle.com/code/andradaolteanu/work-w-audio-data-visualise-classify-recommend)\n",
    "- [Music Genre Classification using Machine Learning](https://www.geeksforgeeks.org/music-genre-classifier-using-machine-learning/)\n",
    "- [Let's tuneðŸŽ§the musicðŸŽµðŸŽ¶with CNNðŸŽ¼XGBoostðŸŽ·ðŸŽ¸ðŸŽ»ðŸŽº](https://www.kaggle.com/code/aishwarya2210/let-s-tune-the-music-with-cnn-xgboost)\n",
    "- [Music Genre Classification: Training an AI model](https://arxiv.org/html/2405.15096v1)\n",
    "- [Music Genre Classification](https://www.kaggle.com/code/jvedarutvija/music-genre-classification)\n",
    "### Dataset\n",
    "We used the GTZAN dataset for our project. The dataset contains 1000 audio tracks, each labeled with a genre. The genres in the dataset are blues, classical, country, disco, hiphop, jazz, metal, pop, reggae, and rock.\n",
    "- [GTZAN dataset](https://www.kaggle.com/andradaolteanu/gtzan-dataset-music-genre-classification)\n",
    "#### Data description\n",
    "- **genres_original:** A collection of 10 genres with 100 audio files each, all having a length of 30 seconds.\n",
    "- **images_original:** A visual representation for each audio file. One way to classify data is through neural networks. Because NNs (like CNN, what we will be using today) usually take in some sort of image representation, the audio files were converted to Mel Spectrograms to make this possible.\n",
    "- **features_30_sec:** Containing features of the audio files. For each song (30 seconds long), a mean and variance were computed over multiple features that can be extracted from an audio file.\n",
    "- **features_3_sec:** Containing the same strutcture as the previous file, but the songs were split before into 3 seconds audio files (this way increasing 10 times the amount of data we fuel into our classification models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technologies\n",
    "We will use the following technologies for our project:\n",
    "- **Python:** We will use Python for data preprocessing, model training, and interface development.\n",
    "- **Jupyter Notebook:** We will use Jupyter Notebook for data exploration, model development, and documentation.\n",
    "\n",
    "### Libraries\n",
    "We will use the following libraries for our project:\n",
    "- **Pandas:** We will use Pandas for data manipulation and analysis.\n",
    "- **NumPy:** We will use NumPy for numerical operations.\n",
    "- **Matplotlib:** We will use Matplotlib for data visualization.\n",
    "- **Seaborn:** We will use Seaborn for data visualization.\n",
    "- **Librosa:** We will use Librosa for audio analysis.\n",
    "- **Ipython:** We will use Ipython for interactive computing.\n",
    "- **Scikit-learn:** We will use Scikit-learn for machine learning.\n",
    "- **Streamlit:** We will use Streamlit for interface development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usual Libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Libraries for feature extraction\n",
    "import librosa as lr\n",
    "import librosa.display\n",
    "\n",
    "# Libraries for audio playing\n",
    "import IPython.display as ipd\n",
    "\n",
    "# Libraries for normalization and dimensionality reduction\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Libraries for splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Libraries for model building and training\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Libraries for model evaluation\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import tree\n",
    "\n",
    "# Libraries for saving models\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization\n",
    "First, we want to gain a better understanding of the dataset. To achieve this, we will visualize the key features extracted from the audio files and compare them. Specifically, we will analyze the distribution of these features across different labels to identify patterns and differences. This will help us determine which features are most relevant for genre classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2D representation of the soundwaves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jazz_sample_path = 'dataset/genres_original/jazz/jazz.00020.wav'\n",
    "jazz_sample, sr = librosa.load(jazz_sample_path)\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveshow(jazz_sample, sr=sr)\n",
    "plt.title('Waveplot for Jazz Music 20')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.xlabel('Time (s)')\n",
    "print(\"Jazz Music 20\")\n",
    "display(ipd.Audio(jazz_sample_path))\n",
    "\n",
    "pop_sample_path = \"dataset/genres_original/pop/pop.00020.wav\"\n",
    "pop_sample, sr = librosa.load(pop_sample_path)\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveshow(pop_sample, sr=sr)\n",
    "plt.title(\"Waveplot for Pop Music 20\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "print(\"Pop Music 20\")\n",
    "display(ipd.Audio(pop_sample_path))\n",
    "\n",
    "rock_sample_path = \"dataset/genres_original/rock/rock.00020.wav\"\n",
    "rock_sample, sr = librosa.load(rock_sample_path)\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveshow(rock_sample, sr=sr)\n",
    "plt.title(\"Waveplot for Rock Music 20\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "print(\"Rock Music 20\")\n",
    "display(ipd.Audio(rock_sample_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mel-frequency cepstral coefficients (MFCCs)\n",
    "MFCCs is a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency. MMFCC are commonly derived as follows:\n",
    "- Take the Fourier transform of (a windowed excerpt of) a signal.\n",
    "- Map the powers of the spectrum obtained above onto the mel scale, using triangular overlapping windows.\n",
    "- Take the logs of the powers at each of the mel frequencies.\n",
    "- Take the discrete cosine transform of the list of mel log powers, as if it were a signal.\n",
    "- The MFCCs are the amplitudes of the resulting spectrum.\n",
    "\n",
    "MFFCCs are widely used in speech and audio processing for tasks such as speech recognition and music genre classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jazz_mfccs = librosa.feature.mfcc(y=jazz_sample, sr=sr)\n",
    "jazz_mfccs_normalized = librosa.util.normalize(jazz_mfccs, axis=1)\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(jazz_mfccs_normalized, x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('MFCC for Jazz Music 20') \n",
    "plt.ylabel('MFCC Coefficients')\n",
    "\n",
    "pop_mfccs = librosa.feature.mfcc(y=pop_sample, sr=sr)\n",
    "pop_mfccs_normalized = librosa.util.normalize(pop_mfccs, axis=1)\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(pop_mfccs_normalized, x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('MFCC for Pop Music 20')\n",
    "plt.ylabel('MFCC Coefficients')\n",
    "\n",
    "rock_mfccs = librosa.feature.mfcc(y=rock_sample, sr=sr)\n",
    "rock_mfccs_normalized = librosa.util.normalize(rock_mfccs, axis=1)\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(rock_mfccs_normalized, x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('MFCC for Rock Music 20')\n",
    "plt.ylabel('MFCC Coefficients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spectral features\n",
    "Spectral features are used to represent the frequency content of an audio signal. They are commonly used in audio analysis to extract information about the timbre and pitch of a sound. Some common spectral features include:\n",
    "- Spectral centroid: Measures how 'bright' a sound is.\n",
    "- Spectral bandwidth: The width of the spectrum.\n",
    "- Spectral rolloff: Indicates the frequency below which most of the energy is concentrated.\n",
    "- Zero-crossing rate: How often the signal changes from positive to negative or vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jazz_centroid = librosa.feature.spectral_centroid(y=jazz_sample, sr=sr)[0]\n",
    "jazz_frames = range(len(jazz_centroid))\n",
    "jazz_t = librosa.frames_to_time(jazz_frames, sr=sr)\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(jazz_t, jazz_centroid, color='b')\n",
    "plt.title('Spectral Centroid for Jazz Music 20')\n",
    "\n",
    "pop_centroid = librosa.feature.spectral_centroid(y=pop_sample, sr=sr)[0]\n",
    "pop_frames = range(len(pop_centroid))\n",
    "pop_t = librosa.frames_to_time(pop_frames, sr=sr)\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(pop_t, pop_centroid, color='b')\n",
    "plt.title('Spectral Centroid for Pop Music 20')\n",
    "\n",
    "rock_centroid = librosa.feature.spectral_centroid(y=rock_sample, sr=sr)[0]\n",
    "rock_frames = range(len(rock_centroid))\n",
    "rock_t = librosa.frames_to_time(rock_frames, sr=sr)\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(rock_t, rock_centroid, color='b')\n",
    "plt.title('Spectral Centroid for Rock Music 20')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jazz_bandwidth = librosa.feature.spectral_bandwidth(y=jazz_sample, sr=sr)[0]\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(jazz_t, jazz_bandwidth, color='g')\n",
    "plt.title('Spectral Bandwidth for Jazz Music 20')\n",
    "\n",
    "pop_bandwidth = librosa.feature.spectral_bandwidth(y=pop_sample, sr=sr)[0]\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(pop_t, pop_bandwidth, color='g')\n",
    "plt.title('Spectral Bandwidth for Pop Music 20')\n",
    "\n",
    "rock_bandwidth = librosa.feature.spectral_bandwidth(y=rock_sample, sr=sr)[0]\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(rock_t, rock_bandwidth, color='g')\n",
    "plt.title('Spectral Bandwidth for Rock Music 20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jazz_rolloff = librosa.feature.spectral_rolloff(y=jazz_sample, sr=sr)[0]\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(jazz_t, jazz_rolloff, color='r')\n",
    "plt.title('Spectral Rolloff for Jazz Music 20')\n",
    "\n",
    "pop_rolloff = librosa.feature.spectral_rolloff(y=pop_sample, sr=sr)[0]\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(pop_t, pop_rolloff, color='r')\n",
    "plt.title('Spectral Rolloff for Pop Music 20')\n",
    "\n",
    "rock_rolloff = librosa.feature.spectral_rolloff(y=rock_sample, sr=sr)[0]\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(rock_t, rock_rolloff, color='r')\n",
    "plt.title('Spectral Rolloff for Rock Music 20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_crossings_jazz = librosa.feature.zero_crossing_rate(jazz_sample)[0]\n",
    "total_crossings_jazz = sum(zero_crossings_jazz)\n",
    "\n",
    "zero_crossings_pop = librosa.feature.zero_crossing_rate(pop_sample)[0]\n",
    "total_crossings_pop = sum(zero_crossings_pop)\n",
    "\n",
    "zero_crossings_rock = librosa.feature.zero_crossing_rate(rock_sample)[0]\n",
    "total_crossings_rock = sum(zero_crossings_rock)\n",
    "\n",
    "genres = ['Jazz', 'Pop', 'Rock']\n",
    "total_crossings = [total_crossings_jazz, total_crossings_pop, total_crossings_rock]\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.bar(genres, total_crossings, color=['blue', 'green', 'red'])\n",
    "plt.title('Total Zero Crossing Rate for Different Genres')\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Total Zero Crossing Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rhythmic features\n",
    "Rhythmic features are used to represent the tempo and rhythm of an audio signal. The estimated beats per minute (BPM) of a song can be used to classify it into different genres, essentially separating fast-paced songs from slow-paced ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jazz_tempo, _ = librosa.beat.beat_track(y=jazz_sample, sr=sr)\n",
    "pop_tempo, _ = librosa.beat.beat_track(y=pop_sample, sr=sr)\n",
    "rock_tempo, _ = librosa.beat.beat_track(y=rock_sample, sr=sr)\n",
    "\n",
    "jazz_tempo_val = (np.mean(jazz_tempo))\n",
    "pop_tempo_val = np.mean(pop_tempo)\n",
    "rock_tempo_val = np.mean(rock_tempo)\n",
    "\n",
    "print(f\"Jazz Tempo: {round(jazz_tempo_val)} BPM\")\n",
    "print(f\"Pop Tempo: {round(pop_tempo_val)} BPM\")\n",
    "print(f\"Rock Tempo: {round(rock_tempo_val)} BPM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chroma features\n",
    "Chroma features refer to the 12 different pitch classes in music. It represents the harmonic and melodic content of a piece of music while ignoring octave differences. The chroma features measure how much energy is present in each pitch class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jazz_chroma = librosa.feature.chroma_stft(y=jazz_sample, sr=sr)\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(jazz_chroma, y_axis='chroma', x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('Chromagram for Jazz Music 20')\n",
    "\n",
    "pop_chroma = librosa.feature.chroma_stft(y=pop_sample, sr=sr)\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(pop_chroma, y_axis='chroma', x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('Chromagram for Pop Music 20')\n",
    "\n",
    "rock_chroma = librosa.feature.chroma_stft(y=rock_sample, sr=sr)\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(rock_chroma, y_axis='chroma', x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('Chromagram for Rock Music 20')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Harmonic and precussive features\n",
    "The harmonic feature in music encompasses its tonal, melodic, and harmonic elements. It includes sustained notes, chords, and melodies produced by instruments such as pianos, guitars, and violins. This component carries the pitch information and forms the melodically resonant part of a song.\n",
    "In contrast, the percussive feature consists of short, transient sounds that drive the rhythm and beat, such as drum hits, cymbals, and other percussive instruments. These elements typically have sharp attacks and play a crucial role in shaping the musicâ€™s rhythmic structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jazz_harmony = librosa.effects.harmonic(y=jazz_sample)\n",
    "jazz_percussive = librosa.effects.percussive(y=jazz_sample)\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(jazz_harmony, color='b')\n",
    "plt.plot(jazz_percussive, color='r')\n",
    "plt.title('Harmonic and Percussive for Jazz Music 20')\n",
    "\n",
    "pop_harmony = librosa.effects.harmonic(y=pop_sample)\n",
    "pop_percussive = librosa.effects.percussive(y=pop_sample)\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(pop_harmony, color='b')\n",
    "plt.plot(pop_percussive, color='r')\n",
    "plt.title('Harmonic and Percussive for Pop Music 20')\n",
    "\n",
    "rock_harmony = librosa.effects.harmonic(y=rock_sample)\n",
    "rock_percussive = librosa.effects.percussive(y=rock_sample)\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(rock_harmony, color='b')\n",
    "plt.plot(rock_percussive, color='r')\n",
    "plt.title('Harmonic and Percussive for Rock Music 20')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "We will preprocess the data to prepare it for model training.\n",
    "First, we will load the csv file containing the audio features. We will drop unnecessary columns and check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_data = pd.read_csv('dataset/features_3_sec.csv')\n",
    "music_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation matrix\n",
    "We will create a correlation matrix to identify the relationships between the features. This will help us understand how the features are related to each other and which features are most relevant for genre classification.\n",
    "Observations:\n",
    "- The diagonal of the correlation matrix shows the correlation of each feature with itself, which is always 1.\n",
    "- The correlation matrix is symmetric, as the correlation between two features is the same regardless of the order.\n",
    "- The first group of features (chroma_stft, rmse, spectral_centroid, spectral_bandwidth, rolloff, zero_crossing_rate) are positively correlated with each other.\n",
    "- The second group of features (mfcc1 to mfcc20) show an interesting pattern where the adjacent features are highly correlated with each other.\n",
    "- there is a noticeable negative correlation between the mfcc2 and the first group of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_cols = [col for col in music_data.columns if 'mean' in col]\n",
    "corr = music_data[spike_cols].corr()\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(corr, ax=ax)\n",
    "plt.title('Correlation between Mean Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal component analysis (PCA)\n",
    "To visualize the data in a lower-dimensional space, we will use PCA to reduce the dimensionality of the features while preserving as much information as possible. PCA transforms the data into a new coordinate system where the main patterns become clearer and variables are less correlated. \n",
    "\n",
    "The process involves standardizing the data, calculating the covariance matrix, computing the eigenvectors and eigenvalues, and projecting the data onto the most important principal components.\n",
    "\n",
    "For standardization, we will use the StandardScaler class from Scikit-learn. This will transform the data so that it has a mean of 0 and a standard deviation of 1.\n",
    "For each feature in the dataset, the StandardScaler performs the following operation:\n",
    "- Calculate the mean of all values in the feature.\n",
    "- Subtract the mean from each value in the feature.\n",
    "- Calculate the standard deviation of all values in the feature.\n",
    "- Divide each value in the feature by the standard deviation.\n",
    "The result is a dataset where each feature has a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "After standardization, we will apply PCA to the data. We will use the PCA class from Scikit-learn to perform PCA.\n",
    "The PCA class works as follows:\n",
    "- The mean of each feature is subtracted from the data.\n",
    "- The covariance matrix of the data is calculated.\n",
    "- The eigenvectors and eigenvalues of the covariance matrix are computed. This is done using the singular value decomposition (SVD) method.\n",
    "- The eigenvectors are sorted by their corresponding eigenvalues in descending order.\n",
    "- The data is projected onto the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = music_data.drop(['filename', 'length', 'label'], axis=1)\n",
    "label = music_data['label']\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "scaled_data = standard_scaler.fit_transform(raw_data)\n",
    "df_scaled_data = pd.DataFrame(scaled_data, columns=raw_data.columns)\n",
    "\n",
    "random.seed(0)\n",
    "random_state = random.randint(0, 1000)\n",
    "\n",
    "pca = PCA(n_components=2, random_state=random_state)\n",
    "pca_scaled_data = pca.fit_transform(df_scaled_data)\n",
    "print(pca.explained_variance_ratio_.sum())\n",
    "print(pca.n_features_in_)\n",
    "pca_df_scaled_data = pd.DataFrame(data=pca_scaled_data, columns=['PC1', 'PC2'])\n",
    "final_df = pd.concat([pca_df_scaled_data, label], axis=1)\n",
    "\n",
    "# Filter the data for selected labels\n",
    "# selected_labels = [\"blues\", \"classical\"]\n",
    "# filtered_df = final_df_standard[final_df_standard[\"label\"].isin(selected_labels)]\n",
    "plt.figure(figsize=(16, 9))\n",
    "sns.scatterplot(x='PC1', y='PC2', data=final_df, hue = 'label')\n",
    "plt.title('PCA on Standard Scaled Data')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "With the data preprocessed, we will train a machine learning model to classify music by genre using the Scikit-learn library (machine learning). Using the features extracted from the audio files, we can try to build a classifier that can accurately predict the genre of a new music tracks.\n",
    "We will split the data into training, testing and validation sets using the train_test_split function from Scikit-learn. This will allow us to train the model on a subset of the data, test it on another subset, and validate it on a third subset. It is important to split the data in this way to avoid overfitting and ensure that the model generalizes well to new data. Our dataset is split into 80% training, 10% testing, and 10% validation sets. The training set is used to train the model, the testing set is used to evaluate the model's performance, and the validation set is used to fine-tune the model's hyperparameters.\n",
    "After splitting the data, we will train the following machine learning models:\n",
    "- Logistic Regression\n",
    "- Stochastic Gradient Descent\n",
    "- Random Forest\n",
    "- Support Vector Machine\n",
    "- K-Nearest Neighbors\n",
    "- XGBoost\n",
    "- Decission Tree\n",
    "- Naive Bayes\n",
    "- Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    df_scaled_data, label, test_size=0.2, random_state=random_state\n",
    ")\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=random_state\n",
    "    )\n",
    "\n",
    "print(f\"Dataset: {len(df_scaled_data)}\")\n",
    "print(f\"Training set: {len(X_train)} ({round(len(X_train)/len(df_scaled_data)*100)}%)\")\n",
    "print(f\"Testing set: {len(X_test)} ({round(len(X_test)/len(df_scaled_data)*100)}%)\")\n",
    "print(f\"Validation set: {len(X_val)} ({round(len(X_val)/len(df_scaled_data)*100)}%)\")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "true_labels = label_encoder.classes_\n",
    "print(\"True Labels: \", true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 200\n",
    "\n",
    "logistic_regression = LogisticRegression(max_iter=max_epochs, random_state=random_state, solver='lbfgs')\n",
    "logistic_regression.fit(X_train, y_train_encoded)\n",
    "\n",
    "train_pred = logistic_regression.predict(X_train)\n",
    "val_pred = logistic_regression.predict(X_val)\n",
    "test_pred = logistic_regression.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train_encoded, train_pred)\n",
    "val_accuracy = accuracy_score(y_val_encoded, val_pred)\n",
    "test_accuracy = accuracy_score(y_test_encoded, test_pred)\n",
    "\n",
    "print(f\"Accuracy on Training Set: {round(train_accuracy * 100, 2)}%\")\n",
    "print(f\"Accuracy on Validation Set: {round(val_accuracy * 100, 2)}%\")\n",
    "print(f\"Accuracy on Testing Set: {round(test_accuracy * 100, 2)}%\")\n",
    "\n",
    "print(f\"Aantal iteraties per klasse: {logistic_regression.n_iter_}\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y_val_encoded, val_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=true_labels, yticklabels=true_labels)\n",
    "plt.title('Confusion Matrix Testing set')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 200\n",
    "learning_rate = 0.01\n",
    "\n",
    "sgd_classifier = SGDClassifier(max_iter=max_epochs, random_state=random_state, learning_rate='constant', eta0=learning_rate)\n",
    "\n",
    "training_scores = []\n",
    "validation_scores = []\n",
    "testing_scores = []\n",
    "epochs = []\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    sgd_classifier.partial_fit(X_train, y_train_encoded, classes=np.unique(y_train_encoded))\n",
    "    train_score = sgd_classifier.score(X_train, y_train_encoded)\n",
    "    val_score = sgd_classifier.score(X_val, y_val_encoded)\n",
    "    test_score = sgd_classifier.score(X_test, y_test_encoded)\n",
    "    training_scores.append(train_score)\n",
    "    validation_scores.append(val_score)\n",
    "    testing_scores.append(test_score)\n",
    "    epochs.append(epoch)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Train accuracy = {train_score:.4f}, Validation accuracy = {val_score:.4f}, Test accuracy = {test_score:.4f}\")\n",
    "\n",
    "train_accuracy = accuracy_score(y_train_encoded, sgd_classifier.predict(X_train))\n",
    "val_accuracy = accuracy_score(y_val_encoded, sgd_classifier.predict(X_val))\n",
    "test_accuracy = accuracy_score(y_test_encoded, sgd_classifier.predict(X_test))\n",
    "\n",
    "print(f\"Accuracy on Training Set: {round(train_accuracy * 100, 2)}%\")\n",
    "print(f\"Accuracy on Validation Set: {round(val_accuracy * 100, 2)}%\")\n",
    "print(f\"Accuracy on Testing Set: {round(test_accuracy * 100, 2)}%\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_encoded, sgd_classifier.predict(X_test))\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=true_labels, yticklabels=true_labels)\n",
    "plt.title('Confusion Matrix Testing set')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(epochs, training_scores, label='Training Accuracy')\n",
    "plt.plot(epochs, validation_scores, label='Validation Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 10\n",
    "n_estimators = 100\n",
    "tree_count = list(range(1, n_estimators + 1, 5))\n",
    "training_scores = []\n",
    "val_scores = []\n",
    "testing_scores = []\n",
    "\n",
    "for trees in tree_count:\n",
    "    random_forest = RandomForestClassifier(n_estimators=trees, max_depth=max_depth, random_state=0)\n",
    "    random_forest.fit(X_train, y_train_encoded)\n",
    "    train_score = random_forest.score(X_train, y_train_encoded)\n",
    "    val_score = random_forest.score(X_val, y_val_encoded)\n",
    "    test_score = random_forest.score(X_test, y_test_encoded)\n",
    "\n",
    "    training_scores.append(train_score)\n",
    "    val_scores.append(val_score)\n",
    "    testing_scores.append(test_score)\n",
    "\n",
    "    if trees % 10 == 1:\n",
    "        print(f\" {trees} trees: Train accuracy = {training_scores[-1]:.4f}, Validation accuracy = {val_scores[-1]:.4f}, Test accuracy = {testing_scores[-1]:.4f}\")\n",
    "\n",
    "max_estimators = tree_count[np.argmax(val_scores)]\n",
    "print(f\"Optimal number of trees: {max_estimators}\")\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=max_estimators, max_depth=max_depth, random_state=random_state)\n",
    "random_forest.fit(X_train, y_train_encoded)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train_encoded, random_forest.predict(X_train))\n",
    "val_accuracy = accuracy_score(y_val_encoded, random_forest.predict(X_val))\n",
    "test_accuracy = accuracy_score(y_test_encoded, random_forest.predict(X_test))\n",
    "\n",
    "print(f\"Accuracy on Training Set: {round(train_accuracy * 100, 2)}%\")\n",
    "print(f\"Accuracy on Validation Set: {round(val_accuracy * 100, 2)}%\")\n",
    "print(f\"Accuracy on Testing Set: {round(test_accuracy * 100, 2)}%\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_encoded, random_forest.predict(X_test))\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=true_labels, yticklabels=true_labels)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(tree_count, training_scores, label=\"Training Accuracy\")\n",
    "plt.plot(tree_count, val_scores, label=\"Validation Accuracy\")\n",
    "plt.title(\"Accuracy vs Number of Trees in Random Forest\")\n",
    "plt.xlabel(\"Number of Trees\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 'rbf'\n",
    "C_values = [0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100]\n",
    "training_scores = []\n",
    "val_scores = []\n",
    "testing_scores = []\n",
    "\n",
    "for C in C_values:\n",
    "    svm_model = SVC(C=C, kernel=kernel, random_state=random_state)\n",
    "    svm_model.fit(X_train, y_train_encoded)\n",
    "    train_score = svm_model.score(X_train, y_train_encoded)\n",
    "    val_score = svm_model.score(X_val, y_val_encoded)\n",
    "    test_score = svm_model.score(X_test, y_test_encoded)\n",
    "\n",
    "    training_scores.append(train_score)\n",
    "    val_scores.append(val_score)\n",
    "    testing_scores.append(test_score)\n",
    "\n",
    "    print(f\"C = {C}: Train accuracy = {train_score:.4f}, Validation accuracy = {val_score:.4f}, Test accuracy = {test_score:.4f}\")\n",
    "\n",
    "best_C = C_values[np.argmax(testing_scores)]\n",
    "print(f\"Best C: {best_C}\")\n",
    "\n",
    "svm_model = SVC(C=best_C, kernel=kernel, random_state=random_state)\n",
    "svm_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train_encoded, svm_model.predict(X_train))\n",
    "val_accuracy = accuracy_score(y_val_encoded, svm_model.predict(X_val))\n",
    "test_accuracy = accuracy_score(y_test_encoded, svm_model.predict(X_test))\n",
    "\n",
    "print(f\"Accuracy on Training Set: {round(train_accuracy * 100, 2)}%\")\n",
    "print(f\"Accuracy on Validation Set: {round(val_accuracy * 100, 2)}%\")\n",
    "print(f\"Accuracy on Testing Set: {round(test_accuracy * 100, 2)}%\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_encoded, svm_model.predict(X_test))\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    conf_matrix, annot=True, fmt=\"d\", xticklabels=true_labels, yticklabels=true_labels\n",
    ")\n",
    "plt.title(\"Confusion Matrix for SVM\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(C_values, training_scores, label=\"Training Accuracy\")\n",
    "plt.plot(C_values, val_scores, label=\"Validation Accuracy\")\n",
    "plt.title(\"Accuracy vs C Parameter in SVM\")\n",
    "plt.xlabel(\"C Parameter\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_neighbors = 20\n",
    "k_values = list(range(1, max_neighbors + 1))\n",
    "weights = 'uniform'\n",
    "\n",
    "training_scores = []\n",
    "val_scores = []\n",
    "testing_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, weights=weights)\n",
    "    knn.fit(X_train, y_train_encoded)\n",
    "    train_score = knn.score(X_train, y_train_encoded)\n",
    "    val_score = knn.score(X_val, y_val_encoded)\n",
    "    test_score = knn.score(X_test, y_test_encoded)\n",
    "\n",
    "    training_scores.append(train_score)\n",
    "    val_scores.append(val_score)\n",
    "    testing_scores.append(test_score)\n",
    "\n",
    "    if k % 2 == 0:\n",
    "        print(f\"K = {k}: Train accuracy = {train_score:.4f}, Validation accuracy = {val_score:.4f}, Test accuracy = {test_score:.4f}\")\n",
    "    \n",
    "best_k = k_values[np.argmax(val_scores[1:]) + 1]\n",
    "print(f\"Best K: {best_k}\")\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=best_k, weights=weights)\n",
    "knn.fit(X_train, y_train_encoded)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train_encoded, knn.predict(X_train))\n",
    "val_accuracy = accuracy_score(y_val_encoded, knn.predict(X_val))\n",
    "test_accuracy = accuracy_score(y_test_encoded, knn.predict(X_test))\n",
    "\n",
    "print(f\"Accuracy on Training Set: {round(train_accuracy * 100, 2)}%\")\n",
    "print(f\"Accuracy on Validation Set: {round(val_accuracy * 100, 2)}%\")\n",
    "print(f\"Accuracy on Testing Set: {round(test_accuracy * 100, 2)}%\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_encoded, knn.predict(X_test))\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=true_labels, yticklabels=true_labels)\n",
    "plt.title('Confusion Matrix for KNN')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(k_values, val_scores, marker='o')\n",
    "plt.title('Validation Accuracy vs Number of Neighbors')\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decission Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = \"entropy\"\n",
    "max_depth = [5, 10, 15, 20, 25]\n",
    "class_weight = 'balanced'\n",
    "\n",
    "training_scores = []\n",
    "val_scores = []\n",
    "testing_scores = []\n",
    "\n",
    "for depth in max_depth:\n",
    "    decision_tree = DecisionTreeClassifier(criterion=criteria, max_depth=depth, class_weight=class_weight, random_state=random_state)\n",
    "    decision_tree.fit(X_train, y_train_encoded)\n",
    "    train_score = decision_tree.score(X_train, y_train_encoded)\n",
    "    val_score = decision_tree.score(X_val, y_val_encoded)\n",
    "    test_score = decision_tree.score(X_test, y_test_encoded)\n",
    "\n",
    "    training_scores.append(train_score)\n",
    "    val_scores.append(val_score)\n",
    "    testing_scores.append(test_score)\n",
    "\n",
    "    print(f\"Max Depth = {depth}: Train accuracy = {train_score:.4f}, Validation accuracy = {val_score:.4f}, Test accuracy = {test_score:.4f}\")\n",
    "\n",
    "best_depth = max_depth[np.argmax(val_scores)]\n",
    "print(f\"Best Max Depth: {best_depth}\")\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(criterion=criteria, max_depth=best_depth, class_weight=class_weight, random_state=random_state)\n",
    "decision_tree.fit(X_train, y_train_encoded)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train_encoded, decision_tree.predict(X_train))\n",
    "val_accuracy = accuracy_score(y_val_encoded, decision_tree.predict(X_val))\n",
    "test_accuracy = accuracy_score(y_test_encoded, decision_tree.predict(X_test))\n",
    "\n",
    "print(f\"Accuracy on Training Set: {round(train_accuracy * 100, 2)}%\")\n",
    "print(f\"Accuracy on Validation Set: {round(val_accuracy * 100, 2)}%\")\n",
    "print(f\"Accuracy on Testing Set: {round(test_accuracy * 100, 2)}%\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_encoded, decision_tree.predict(X_test))\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=true_labels, yticklabels=true_labels)\n",
    "plt.title('Confusion Matrix for Decision Tree')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = \"log_loss\"\n",
    "learning_rate = 0.2\n",
    "n_estimators = 300\n",
    "\n",
    "gradient_boosting = GradientBoostingClassifier(loss=loss, learning_rate=learning_rate, n_estimators=n_estimators, random_state=random_state)\n",
    "gradient_boosting.fit(X_train, y_train_encoded)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train_encoded, gradient_boosting.predict(X_train))\n",
    "val_accuracy = accuracy_score(y_val_encoded, gradient_boosting.predict(X_val))\n",
    "test_accuracy = accuracy_score(y_test_encoded, gradient_boosting.predict(X_test))\n",
    "\n",
    "print(f\"Accuracy on Training Set: {round(train_accuracy * 100, 2)}%\")\n",
    "print(f\"Accuracy on Validation Set: {round(val_accuracy * 100, 2)}%\")\n",
    "print(f\"Accuracy on Testing Set: {round(test_accuracy * 100, 2)}%\")\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test_encoded, gradient_boosting.predict(X_test))\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=true_labels, yticklabels=true_labels)\n",
    "plt.title('Confusion Matrix for Gradient Boosting')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(gradient_boosting.train_score_)\n",
    "plt.title('Loss over Iterations')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the model based on performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 'rbf'\n",
    "C_value = 20\n",
    "final_svm_model = SVC(C=C_value, kernel=kernel, random_state=random_state)\n",
    "final_model = final_svm_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "print(f\"The final model is a SVM model with C = {C_value} and kernel = {kernel}\")\n",
    "print(f\"The score of the model on the trained data is {round(final_model.score(X_train, y_train_encoded)*100,2)}\")\n",
    "print(f\"The score of the model on the validation data is {round(final_model.score(X_val, y_val_encoded)*100,2)}\")\n",
    "print(f\"The score of the model on the test data is {round(final_model.score(X_test, y_test_encoded)*100,2)}\")\n",
    "\n",
    "print(f\"The name of the features during fit {final_model.feature_names_in_}\")\n",
    "print(f\"The indices of support vectors {final_model.support_}\")\n",
    "print(f\"The number of support vectors for each class {final_model.n_support_}\")\n",
    "\n",
    "filename = 'svm_final_model.sav'\n",
    "pickle.dump(final_model, open(filename, 'wb'))\n",
    "print(f\"Model saved as {filename}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
